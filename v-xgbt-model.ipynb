{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:50.757643Z",
     "iopub.status.busy": "2020-10-12T13:56:50.756898Z",
     "iopub.status.idle": "2020-10-12T13:56:50.763686Z",
     "shell.execute_reply": "2020-10-12T13:56:50.762889Z"
    },
    "papermill": {
     "duration": 0.029896,
     "end_time": "2020-10-12T13:56:50.763858",
     "exception": false,
     "start_time": "2020-10-12T13:56:50.733962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/costa-rican-household-poverty-prediction/test.csv.zip\n",
      "/kaggle/input/costa-rican-household-poverty-prediction/sample_submission.csv\n",
      "/kaggle/input/costa-rican-household-poverty-prediction/test.csv\n",
      "/kaggle/input/costa-rican-household-poverty-prediction/train.csv.zip\n",
      "/kaggle/input/costa-rican-household-poverty-prediction/codebook.csv\n",
      "/kaggle/input/costa-rican-household-poverty-prediction/train.csv\n",
      "/kaggle/input/costa-rican-household-poverty-prediction/sample_submission.csv.zip\n",
      "/kaggle/input/costa-rican-household-poverty-prediction/codebook.xlsx\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "#import numpy as np # linear algebra\n",
    "#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015621,
     "end_time": "2020-10-12T13:56:50.796139",
     "exception": false,
     "start_time": "2020-10-12T13:56:50.780518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:50.836528Z",
     "iopub.status.busy": "2020-10-12T13:56:50.835775Z",
     "iopub.status.idle": "2020-10-12T13:56:53.336114Z",
     "shell.execute_reply": "2020-10-12T13:56:53.335317Z"
    },
    "papermill": {
     "duration": 2.52445,
     "end_time": "2020-10-12T13:56:53.336237",
     "exception": false,
     "start_time": "2020-10-12T13:56:50.811787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "import matplotlib.cm as cm\n",
    "#sb.set() # set the default Seaborn style for graphics\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:53.373368Z",
     "iopub.status.busy": "2020-10-12T13:56:53.372618Z",
     "iopub.status.idle": "2020-10-12T13:56:53.375319Z",
     "shell.execute_reply": "2020-10-12T13:56:53.375878Z"
    },
    "papermill": {
     "duration": 0.023584,
     "end_time": "2020-10-12T13:56:53.376040",
     "exception": false,
     "start_time": "2020-10-12T13:56:53.352456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:53.417326Z",
     "iopub.status.busy": "2020-10-12T13:56:53.416510Z",
     "iopub.status.idle": "2020-10-12T13:56:53.613296Z",
     "shell.execute_reply": "2020-10-12T13:56:53.612608Z"
    },
    "papermill": {
     "duration": 0.221241,
     "end_time": "2020-10-12T13:56:53.613432",
     "exception": false,
     "start_time": "2020-10-12T13:56:53.392191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/train.csv')\n",
    "# Drop target data\n",
    "train_target = train_data['Target']\n",
    "train_data.drop(['Target'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:53.652602Z",
     "iopub.status.busy": "2020-10-12T13:56:53.651691Z",
     "iopub.status.idle": "2020-10-12T13:56:53.655060Z",
     "shell.execute_reply": "2020-10-12T13:56:53.654405Z"
    },
    "papermill": {
     "duration": 0.025411,
     "end_time": "2020-10-12T13:56:53.655181",
     "exception": false,
     "start_time": "2020-10-12T13:56:53.629770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016063,
     "end_time": "2020-10-12T13:56:53.687626",
     "exception": false,
     "start_time": "2020-10-12T13:56:53.671563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Feature Engineering based on Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:53.764170Z",
     "iopub.status.busy": "2020-10-12T13:56:53.763182Z",
     "iopub.status.idle": "2020-10-12T13:56:53.766473Z",
     "shell.execute_reply": "2020-10-12T13:56:53.765865Z"
    },
    "papermill": {
     "duration": 0.062551,
     "end_time": "2020-10-12T13:56:53.766597",
     "exception": false,
     "start_time": "2020-10-12T13:56:53.704046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check columns with nan values and their nan values counts - missing data\n",
    "def findNanCol(orig_df,print_flag=False):\n",
    "    df_missing = orig_df.isnull()\n",
    "    col_list_missing_data = []\n",
    "    for col in df_missing.columns.values.tolist():\n",
    "        try:\n",
    "            x=df_missing[col].value_counts()[1]\n",
    "            if print_flag:\n",
    "                print(\"Column {}: {} missing value counts\".format(col,x))\n",
    "            col_list_missing_data.append(col)\n",
    "        except:\n",
    "            continue\n",
    "    return col_list_missing_data\n",
    "\n",
    "\n",
    "# Get all col with type Objects to check if there more than one type and for labelencoder\n",
    "def findObjectTypeCol(orig_df,print_flag=False):\n",
    "    obj_col_list = []\n",
    "    for col in orig_df.columns.values.tolist():\n",
    "        if orig_df[col].dtypes == \"object\":\n",
    "            if print_flag:\n",
    "                print(col)\n",
    "            obj_col_list.append(col)\n",
    "    return obj_col_list\n",
    "\n",
    "# Fill NA values in dataset appropriately\n",
    "def fillMissingValues(train_data):\n",
    "    \n",
    "    # Fill the nans in v2a1 with 0s as most have paid for their own house and thus no rent payment.\n",
    "    train_data['v2a1'] = train_data['v2a1'].fillna(0).astype('float64')\n",
    "    \n",
    "    # Fill nan with 0s as only v18q with 0s have nan values for v18q1\n",
    "    # Furthermore, we will get the avg num of laptops per household member which is a better feature.\n",
    "    # The new feature will be created in 3rd stage of feature engineering.\n",
    "    train_data['v18q1'] = train_data['v18q1'].fillna(0).astype('float64')\n",
    "    \n",
    "    # Fill nans in rez_esc with zeros as this feature is meant for age between 7 and 19 as per definition.\n",
    "    train_data['rez_esc'] = train_data['rez_esc'].fillna(0).astype('float64')\n",
    "    \n",
    "    # It turns out that the number of household members 18+ is zero which have givesn the nan value.\n",
    "    # Therefore, we will convert \"meanedu\" values to zero.\n",
    "    train_data['meaneduc'] = train_data['meaneduc'].fillna(0).astype('float64')\n",
    "    train_data['SQBmeaned'] = train_data['SQBmeaned'].fillna(0).astype('float64')\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "# Calculate depenedncy rate for those missing values\n",
    "def getDependencyRate(train_data):\n",
    "    for i,val in enumerate(train_data['dependency']):\n",
    "        if val == 'no':\n",
    "            train_data.loc[i,'dependency'] = 0\n",
    "        elif val == 'yes':\n",
    "            total_num = float(train_data.loc[i,'hogar_total'])\n",
    "            num_abv_65_and_below_19 = float(train_data.loc[i,'hogar_nin']) + float(train_data.loc[i,'hogar_mayor'])\n",
    "            train_data.loc[i,'dependency'] = num_abv_65_and_below_19/(total_num - num_abv_65_and_below_19)\n",
    "\n",
    "    train_data['dependency'] = train_data['dependency'].astype('float64')\n",
    "    return train_data\n",
    "\n",
    "# Encodes features with more than one data type\n",
    "def encoder(train_data):\n",
    "    dic = {'yes' : '1', 'no' : '0'}\n",
    "    \n",
    "    train_data.drop(['Id'],axis=1,inplace=True)\n",
    "    train_data['idhogar'] = LabelEncoder().fit_transform(train_data['idhogar'])\n",
    "    train_data = getDependencyRate(train_data)\n",
    "    train_data['edjefe']= train_data['edjefe'].replace(dic).astype('float64')\n",
    "    train_data['edjefa'] = train_data['edjefa'].replace(dic).astype('float64')\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "# Converts str type columns to float\n",
    "def convertToFloat(train_data,obj_col_list,col_list_missing_data):\n",
    "    obj_col_list.extend(col_list_missing_data)\n",
    "    for col in train_data.columns.values.tolist():\n",
    "        if col not in obj_col_list:\n",
    "            train_data[col] = train_data[col].astype('float64')\n",
    "    return train_data\n",
    "\n",
    "# Adds and modifies new features\n",
    "def addAndModifyFeatures(train_data):\n",
    "    ### Definition:\n",
    "    # r4t3: Total number of individuals in houshold including domestic employees/friends/tenants.\n",
    "    # hogar_total = tamgog = hhsize: Total number of houshold members excluding domestic employees/friends/tenants.\n",
    "    # tamviv: Unclear definition so will be dropped.\n",
    "    \n",
    "    ### Add new features\n",
    "    \n",
    "    train_data['v2a1_per_room'] = train_data['v2a1']/train_data['rooms']\n",
    "    # It is logical for a household to account only for tables among household members.\n",
    "    train_data['v18q1_per_household_member'] = train_data['v18q1']/train_data['hogar_total']\n",
    "    # It is logical for a household to account only for mobile phones among household members.\n",
    "    train_data['qmobileph_per_household_member'] = train_data['qmobilephone']/train_data['hogar_total']\n",
    "    # It is logical for a household to account only for rooms among household members. Having non\n",
    "    # household members to stay is subjective and not considered as essentials.\n",
    "    train_data['rooms_per_household_member'] = train_data['rooms']/train_data['hogar_total']\n",
    "     # It is logical for a household to account only for bedrooms among household members. Having non\n",
    "    # household members to stay is subjective and not considered as essentials.\n",
    "    train_data['bedroom_per_household_member'] = train_data['bedrooms']/train_data['hogar_total']\n",
    "    # Number of non household members in the house(domestic employees/friends/tenants)\n",
    "    train_data['Num_of_non_household_members'] = train_data['r4t3']-train_data['hogar_total']\n",
    "    # Proportion of household adults aged btw 19 and 65 ---> (adults-old_aged)/total\n",
    "    train_data['hogar_adul_btw_19_and_65'] = (train_data['hogar_adul']-train_data['hogar_mayor'])/train_data['hogar_total']\n",
    "    \n",
    "    \n",
    "    ### Modify to obtain proportion features\n",
    "    \n",
    "    # The feature values below includes non houshold mmbers as mentioned in data exploration notebook.\n",
    "    # Therefore, the values are divided by total number of people in house.\n",
    "    train_data['r4h1'] = train_data['r4h1']/train_data['r4t3']\n",
    "    train_data['r4h2'] = train_data['r4h2']/train_data['r4t3']\n",
    "    train_data['r4h3'] = train_data['r4h3']/train_data['r4t3']\n",
    "    train_data['r4m1'] = train_data['r4m1']/train_data['r4t3']\n",
    "    train_data['r4m2'] = train_data['r4m2']/train_data['r4t3']\n",
    "    train_data['r4m3'] = train_data['r4m3']/train_data['r4t3']\n",
    "    train_data['r4t1'] = train_data['r4t1']/train_data['r4t3']\n",
    "    train_data['r4t2'] = train_data['r4t2']/train_data['r4t3']\n",
    "\n",
    "    # The feature values below excludes non houshold mmbers as verified in data exploration notebook.\n",
    "    # Therefore, the values are divided by number of houehold members.\n",
    "    train_data['hogar_nin'] = train_data['hogar_nin']/train_data['hogar_total']\n",
    "    train_data['hogar_adul'] = train_data['hogar_adul']/train_data['hogar_total']\n",
    "    train_data['hogar_mayor'] = train_data['hogar_mayor']/train_data['hogar_total'] \n",
    "    \n",
    "    ### Remove feature col as some are duplicates and \n",
    "    # most of these col are total number which is included in proportion calculation\n",
    "    train_data.drop(['idhogar','v18q1','rooms','bedrooms','r4t3','hogar_total','agesq','hhsize','tamviv','tamhog'],axis=1,inplace=True)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:53.820837Z",
     "iopub.status.busy": "2020-10-12T13:56:53.808986Z",
     "iopub.status.idle": "2020-10-12T13:56:53.824246Z",
     "shell.execute_reply": "2020-10-12T13:56:53.823606Z"
    },
    "papermill": {
     "duration": 0.041016,
     "end_time": "2020-10-12T13:56:53.824369",
     "exception": false,
     "start_time": "2020-10-12T13:56:53.783353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applies feature engineering in one function\n",
    "def applyFE(train_data):\n",
    "    # List of columns that have missing values and their corresponding number of missing value counts\n",
    "    col_list_missing_data = findNanCol(train_data)\n",
    "    \n",
    "    # List of columns that needs to be verified as they in object type\n",
    "    obj_col_list = findObjectTypeCol(train_data)\n",
    "    \n",
    "    # Convert columns not having missing data or not having many object types to float\n",
    "    train_data = convertToFloat(train_data,obj_col_list,col_list_missing_data)\n",
    "    \n",
    "    ### STAGE 1\n",
    "    ### Dealing with features with missing values.\n",
    "    # Fills missing values\n",
    "    train_data = fillMissingValues(train_data)\n",
    "    \n",
    "    ### STAGE 2\n",
    "    ### Dealing with features with object types.\n",
    "    # Encodes features with more than one data type\n",
    "    train_data = encoder(train_data)\n",
    "    \n",
    "    ### STAGE 3\n",
    "    ### Dealing with new and modified features.\n",
    "    # Encodes features with more than one data type\n",
    "    train_data = addAndModifyFeatures(train_data)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "# Get components of predicted labels\n",
    "def getConstituentsOfPredicted(cf,pred_label,angle=45):\n",
    "\n",
    "    # cf[actual-1][predicted-1]\n",
    "    p13 = cf[0][pred_label-1]\n",
    "    p23 = cf[1][pred_label-1]\n",
    "    p33 = cf[2][pred_label-1]\n",
    "    p43 = cf[3][pred_label-1]\n",
    "\n",
    "    # Data to plot\n",
    "    pred_labels = ['extreme poverty', 'moderate poverty', 'vulnerable households','non vulnerable households']\n",
    "    sizes = [p13, p23, p33, p43]\n",
    "    colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "\n",
    "    # Plot\n",
    "    plt.pie(sizes, labels=pred_labels, colors=colors, autopct='%1.0f%%', shadow=False, startangle=angle, textprops={'fontsize': 14})\n",
    "    plt.title(\"Constituents of Costa Rican Household Poverty Levels predicted as \"+pred_labels[pred_label-1],fontsize=20)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    return p13,p23,p33,p43\n",
    "\n",
    "# Get components of actual labels\n",
    "def getPredictionOfActualLabels(p,cf,actual_label,angle=45):\n",
    "    \n",
    "    # Data to plot\n",
    "    labels = ['extreme poverty', 'moderate poverty', 'vulnerable households','non vulnerable households']\n",
    "    sizes = [p[0], p[1], p[2], p[3]]\n",
    "    colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "\n",
    "    # Plot\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', shadow=False, startangle=angle, textprops={'fontsize': 14})\n",
    "    plt.title(\"Predictions of Costa Rican Household Poverty Levels with actual labels as \"+labels[actual_label-1],fontsize=20)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:53.869033Z",
     "iopub.status.busy": "2020-10-12T13:56:53.868274Z",
     "iopub.status.idle": "2020-10-12T13:56:56.412562Z",
     "shell.execute_reply": "2020-10-12T13:56:56.411740Z"
    },
    "papermill": {
     "duration": 2.571368,
     "end_time": "2020-10-12T13:56:56.412687",
     "exception": false,
     "start_time": "2020-10-12T13:56:53.841319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engineering for train data\n",
    "train_data = applyFE(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:56.452206Z",
     "iopub.status.busy": "2020-10-12T13:56:56.451438Z",
     "iopub.status.idle": "2020-10-12T13:56:56.458740Z",
     "shell.execute_reply": "2020-10-12T13:56:56.458083Z"
    },
    "papermill": {
     "duration": 0.028993,
     "end_time": "2020-10-12T13:56:56.458887",
     "exception": false,
     "start_time": "2020-10-12T13:56:56.429894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = train_data.copy()\n",
    "y_train = train_target.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016901,
     "end_time": "2020-10-12T13:56:56.493134",
     "exception": false,
     "start_time": "2020-10-12T13:56:56.476233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Model: XGBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T13:56:56.537392Z",
     "iopub.status.busy": "2020-10-12T13:56:56.536610Z",
     "iopub.status.idle": "2020-10-12T14:04:47.665611Z",
     "shell.execute_reply": "2020-10-12T14:04:47.666320Z"
    },
    "papermill": {
     "duration": 471.156239,
     "end_time": "2020-10-12T14:04:47.666523",
     "exception": false,
     "start_time": "2020-10-12T13:56:56.510284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None, colsample_bytree=1,\n",
       "                                     gamma=0, gpu_id=None,\n",
       "                                     importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=0.15, max_delta_step=None,\n",
       "                                     max_depth=35, min_child_weight=1,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=200, n_jobs=None, num_class=4,\n",
       "                                     num_parallel_tree=None,\n",
       "                                     objective='multi:softmax',\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=0.3, scale_pos_weight=None,\n",
       "                                     subsample=0.8, tree_method=None,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             param_grid={'gamma': [0, 0.3], 'reg_lambda': [0.3, 0.35]},\n",
       "             scoring=make_scorer(f1_score, average=weighted))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        #'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0,0.3],\n",
    "        #'subsample': [0.75, 0.8, 1.0],\n",
    "        #'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "        #'max_depth': [30, 35, 40],\n",
    "        #'learning_rate': [0.1,0.15,0.2],\n",
    "        'reg_lambda':[0.3,0.35],\n",
    "        }\n",
    "\n",
    "# Specify a scoring method\n",
    "scorer = metrics.make_scorer(metrics.f1_score, average = 'weighted')\n",
    "# Model initializer\n",
    "xgb_c = xgb.XGBClassifier(objective = 'multi:softmax', num_class = 4, gamma = 0, colsample_bytree = 1, subsample=0.8, learning_rate = 0.15, max_depth = 35, min_child_weight = 1, n_estimators = 200, reg_lambda = 0.3)\n",
    "# GridSearch\n",
    "xgb_cv = GridSearchCV(estimator = xgb_c, param_grid = params, scoring = scorer, cv = 3)\n",
    "# Fit train data\n",
    "xgb_cv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:04:47.717578Z",
     "iopub.status.busy": "2020-10-12T14:04:47.716407Z",
     "iopub.status.idle": "2020-10-12T14:04:47.724183Z",
     "shell.execute_reply": "2020-10-12T14:04:47.725033Z"
    },
    "papermill": {
     "duration": 0.03768,
     "end_time": "2020-10-12T14:04:47.725226",
     "exception": false,
     "start_time": "2020-10-12T14:04:47.687546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for training data: 0.45003848842853317 \n",
      "\n",
      "Best parameters: {'gamma': 0.3, 'reg_lambda': 0.35} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## View the accuracy score\n",
    "print('Best score for training data:', xgb_cv.best_score_,\"\\n\") \n",
    "\n",
    "## View the best parameters for the model found using grid search\n",
    "print('Best parameters:',xgb_cv.best_params_,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:04:47.768460Z",
     "iopub.status.busy": "2020-10-12T14:04:47.767368Z",
     "iopub.status.idle": "2020-10-12T14:04:47.770932Z",
     "shell.execute_reply": "2020-10-12T14:04:47.770288Z"
    },
    "papermill": {
     "duration": 0.026315,
     "end_time": "2020-10-12T14:04:47.771054",
     "exception": false,
     "start_time": "2020-10-12T14:04:47.744739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model initializer with best param values\n",
    "#xgb_c = xgb.XGBClassifier(objective = 'multi:softmax', num_class = 4, gamma = 0.1, colsample_bytree = 1, subsample=0.8, learning_rate = 0.2, max_depth = 16, min_child_weight = 1, n_estimators = 10, reg_lambda = 0.1, random_state=0)\n",
    "# Fit train data\n",
    "#xgb_c.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:04:47.818550Z",
     "iopub.status.busy": "2020-10-12T14:04:47.817715Z",
     "iopub.status.idle": "2020-10-12T14:04:48.088190Z",
     "shell.execute_reply": "2020-10-12T14:04:48.089140Z"
    },
    "papermill": {
     "duration": 0.299497,
     "end_time": "2020-10-12T14:04:48.089385",
     "exception": false,
     "start_time": "2020-10-12T14:04:47.789888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for rf model XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1, gamma=0, gpu_id=None,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=35,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_class=4,\n",
      "              num_parallel_tree=None, objective='multi:softmax',\n",
      "              random_state=None, reg_alpha=None, reg_lambda=0.3,\n",
      "              scale_pos_weight=None, subsample=0.8, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       755\n",
      "           2       1.00      1.00      1.00      1597\n",
      "           3       1.00      1.00      1.00      1209\n",
      "           4       1.00      1.00      1.00      5996\n",
      "\n",
      "    accuracy                           1.00      9557\n",
      "   macro avg       1.00      1.00      1.00      9557\n",
      "weighted avg       1.00      1.00      1.00      9557\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate for train data\n",
    "y_pred_train = xgb_cv.predict(x_train)\n",
    "print(\"Classification report for rf model %s:\\n%s\\n\"\n",
    "      % (xgb_c, metrics.classification_report(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022498,
     "end_time": "2020-10-12T14:04:48.136049",
     "exception": false,
     "start_time": "2020-10-12T14:04:48.113551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submit for XGBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:04:48.193405Z",
     "iopub.status.busy": "2020-10-12T14:04:48.192531Z",
     "iopub.status.idle": "2020-10-12T14:04:57.918875Z",
     "shell.execute_reply": "2020-10-12T14:04:57.918158Z"
    },
    "papermill": {
     "duration": 9.759209,
     "end_time": "2020-10-12T14:04:57.919009",
     "exception": false,
     "start_time": "2020-10-12T14:04:48.159800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "# Read the test CSV Data\n",
    "path = '/kaggle/input/costa-rican-household-poverty-prediction/test.csv'\n",
    "test_data = pd.read_csv(path)\n",
    "test_data = applyFE(test_data)\n",
    "y_pred = xgb_cv.predict(test_data)\n",
    "# Read submission file\n",
    "path = '/kaggle/input/costa-rican-household-poverty-prediction/sample_submission.csv'\n",
    "test = pd.read_csv(path)\n",
    "test['Target'] = y_pred\n",
    "test.to_csv(\"submission.csv\", index= False)\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.01904,
     "end_time": "2020-10-12T14:04:57.957731",
     "exception": false,
     "start_time": "2020-10-12T14:04:57.938691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 492.304606,
   "end_time": "2020-10-12T14:04:58.085539",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-12T13:56:45.780933",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
